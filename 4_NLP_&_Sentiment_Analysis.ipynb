{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbbb8dc8",
   "metadata": {},
   "source": [
    "# NLP & Sentiment Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41d782",
   "metadata": {},
   "source": [
    "En esta etapa del proyecto manejaremos diferentes modelos pre-entrenados (librerías) y trataremos de evaluar cuál es el que nos ofrece mejores resultados.\n",
    "\n",
    "Evidentemente, sería conveniente desarrollar un modelo propio ad-hoc según las necesidades particulares del proyecto, lo cual probablemente nos permita alcanzar resultados más precisos. Sin embargo, no debemos olvidar que por el momento pretendemos construir las bases del proyecto y obtener un mínimo producto viable que demuestre su potencial. Disponemos de muy poco tiempo como para desarrollar y entrenar nuestro propio modelo y es por eso que se opta por la utilización de modelos pre-entrenados de NLP a la hora de analizar el sentimiento de los tweets y, por tanto, la percepción del usuario respecto de las marcas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f90ff",
   "metadata": {},
   "source": [
    "### TextBlob (spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2ab98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INTRO!!!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''INTRO!!!'''\n",
    "\n",
    "# Esplicación breve.\n",
    "\n",
    "# Catarse bien de cómo funciona.\n",
    "# Ojo instalaciones (lo he metido en todos laos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fd2ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['I', 'had', 'a']),\n",
       " WordList(['had', 'a', 'really']),\n",
       " WordList(['a', 'really', 'horrible']),\n",
       " WordList(['really', 'horrible', 'day']),\n",
       " WordList(['horrible', 'day', 'It']),\n",
       " WordList(['day', 'It', 'was']),\n",
       " WordList(['It', 'was', 'the']),\n",
       " WordList(['was', 'the', 'worst']),\n",
       " WordList(['the', 'worst', 'day']),\n",
       " WordList(['worst', 'day', 'ever']),\n",
       " WordList(['day', 'ever', 'But']),\n",
       " WordList(['ever', 'But', 'every']),\n",
       " WordList(['But', 'every', 'now']),\n",
       " WordList(['every', 'now', 'and']),\n",
       " WordList(['now', 'and', 'then']),\n",
       " WordList(['and', 'then', 'I']),\n",
       " WordList(['then', 'I', 'have']),\n",
       " WordList(['I', 'have', 'a']),\n",
       " WordList(['have', 'a', 'really']),\n",
       " WordList(['a', 'really', 'good']),\n",
       " WordList(['really', 'good', 'day']),\n",
       " WordList(['good', 'day', 'that']),\n",
       " WordList(['day', 'that', 'makes']),\n",
       " WordList(['that', 'makes', 'me']),\n",
       " WordList(['makes', 'me', 'happy'])]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "text = 'I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.'\n",
    "doc = nlp(text)\n",
    "doc._.blob.polarity                            # Polarity: -0.125\n",
    "doc._.blob.subjectivity                        # Subjectivity: 0.9\n",
    "doc._.blob.sentiment_assessments.assessments   # Assessments: [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n",
    "doc._.blob.ngrams()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7bca71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.125\n",
      "0.9\n",
      "[(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I had a really horrible day. It was the worst day ever! But every now and then I have a really good day that makes me happy.\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "print(blob.sentiment_assessments.polarity)\n",
    "# -0.125\n",
    "\n",
    "print(blob.sentiment_assessments.subjectivity)\n",
    "# 0.9\n",
    "\n",
    "print(blob.sentiment_assessments.assessments)\n",
    "# [(['really', 'horrible'], -1.0, 1.0, None), (['worst', '!'], -1.0, 1.0, None), (['really', 'good'], 0.7, 0.6000000000000001, None), (['happy'], 0.8, 1.0, None)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7d87b",
   "metadata": {},
   "source": [
    "### VADER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16059b7",
   "metadata": {},
   "source": [
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) es una librería utilizada para el análisis del sentimiento que se enfoca en los textos de social media. Así, pone énfases en las rules que captan la esencia del texto que normalmente se ve en las redes sociales. Algo interesante de VADER es que está pensado para poder actuar sobre texto sobre el que se ha hecho un limpieza muy básica (conservando emojis, signos de exclamación, etc.). Nosotros lo probaremos sobre los datos ya limpios ya que confiamos en que ofrecerán un mejor resultado (aunque no se presenta el proceso, se ha probado con los datos originales en bruto, pero no consique analizar los sentimientos correctamente y la gran mayoría de scores para negatividad, neutralidad y positivadad son 0). No obstante, lo ideal sería evaluar la precisión del análisis sobre tweets con diferentes niveles de limpieza (desde el más básico, al más detallista) y comparar los modelos para identificar el que ofrece un mayor rendimiento (incluyendo también los análisis desarrollados mediante otros modelos diferentes a VADER)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632ebf11",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "230473dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nlp_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8519d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "brands_tweets = pd.read_pickle('data/brands_tokens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10dbe050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>brand_attribute</th>\n",
       "      <th>brand</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nobody cares about nike in russia russia is al...</td>\n",
       "      <td>[]</td>\n",
       "      <td>quality</td>\n",
       "      <td>nike</td>\n",
       "      <td>[care, nike, russia, russia, adidas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ye green nike hoodie waala two weeks pehle put...</td>\n",
       "      <td>[]</td>\n",
       "      <td>quality</td>\n",
       "      <td>nike</td>\n",
       "      <td>[ye, green, nike, hoodie, waala, week, pehle, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nike okundaye also knowns nike twins seven sev...</td>\n",
       "      <td>['#womengiant', '#Documentwomen']</td>\n",
       "      <td>quality</td>\n",
       "      <td>nike</td>\n",
       "      <td>[nike, okundaye, knowns, nike, twin, seven, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day four of maxmadness air max month nike air ...</td>\n",
       "      <td>['#maxmadness', '#AirMaxMonth', '#airmaxgang',...</td>\n",
       "      <td>quality</td>\n",
       "      <td>nike</td>\n",
       "      <td>[day, maxmadness, air, max, month, nike, air, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thank you sir</td>\n",
       "      <td>[]</td>\n",
       "      <td>quality</td>\n",
       "      <td>nike</td>\n",
       "      <td>[thank, sir]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  nobody cares about nike in russia russia is al...   \n",
       "1  ye green nike hoodie waala two weeks pehle put...   \n",
       "2  nike okundaye also knowns nike twins seven sev...   \n",
       "3  day four of maxmadness air max month nike air ...   \n",
       "4                                      thank you sir   \n",
       "\n",
       "                                            hashtags brand_attribute brand  \\\n",
       "0                                                 []         quality  nike   \n",
       "1                                                 []         quality  nike   \n",
       "2                  ['#womengiant', '#Documentwomen']         quality  nike   \n",
       "3  ['#maxmadness', '#AirMaxMonth', '#airmaxgang',...         quality  nike   \n",
       "4                                                 []         quality  nike   \n",
       "\n",
       "                                               token  \n",
       "0               [care, nike, russia, russia, adidas]  \n",
       "1  [ye, green, nike, hoodie, waala, week, pehle, ...  \n",
       "2  [nike, okundaye, knowns, nike, twin, seven, se...  \n",
       "3  [day, maxmadness, air, max, month, nike, air, ...  \n",
       "4                                       [thank, sir]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brands_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31929e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_tweets['vader_scores'] = brands_tweets.token.apply(vader_scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1477bed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>brand_attribute</th>\n",
       "      <th>brand</th>\n",
       "      <th>token</th>\n",
       "      <th>vader_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26586</th>\n",
       "      <td>the nike go flyease surfaces in another multic...</td>\n",
       "      <td>['#sneakersapp']</td>\n",
       "      <td>quality</td>\n",
       "      <td>nike</td>\n",
       "      <td>[nike, flyease, surface, multicolor, sneakersapp]</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7967</th>\n",
       "      <td>just in nike streetgato white lime glow</td>\n",
       "      <td>[]</td>\n",
       "      <td>quality</td>\n",
       "      <td>nike</td>\n",
       "      <td>[nike, streetgato, white, lime, glow]</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>for the first time the fruity pebbles lebron f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>quality</td>\n",
       "      <td>nike</td>\n",
       "      <td>[time, fruity, pebble, lebron, set, retail, re...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4839</th>\n",
       "      <td>nike ikea close russian stores as sanctions tr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>quality</td>\n",
       "      <td>nike</td>\n",
       "      <td>[nike, ikea, close, russian, store, sanction, ...</td>\n",
       "      <td>{'neg': 0.208, 'neu': 0.792, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>sizes up to one two for the vast grey electric...</td>\n",
       "      <td>[]</td>\n",
       "      <td>price</td>\n",
       "      <td>nike</td>\n",
       "      <td>[size, vast, grey, electric, green, nike, spac...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.809, 'pos': 0.191, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47081</th>\n",
       "      <td>so dumb it s ike when people were burning thei...</td>\n",
       "      <td>[]</td>\n",
       "      <td>quality</td>\n",
       "      <td>adidas</td>\n",
       "      <td>[dumb, ike, people, burning, nike, gear, like,...</td>\n",
       "      <td>{'neg': 0.525, 'neu': 0.35, 'pos': 0.125, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41527</th>\n",
       "      <td>my number one actually wore em to see batman t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>quality</td>\n",
       "      <td>adidas</td>\n",
       "      <td>[number, actually, wore, em, batman, tuesday, ...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.645, 'pos': 0.355, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38498</th>\n",
       "      <td>so good i had to share check out all the items...</td>\n",
       "      <td>['#poshmark', '#fashion', '#style', '#shopmycl...</td>\n",
       "      <td>quality</td>\n",
       "      <td>adidas</td>\n",
       "      <td>[good, share, check, item, loving, poshmark, f...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.383, 'pos': 0.617, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38801</th>\n",
       "      <td>ad dropped via nike us nike air sesh white var...</td>\n",
       "      <td>[]</td>\n",
       "      <td>quality</td>\n",
       "      <td>adidas</td>\n",
       "      <td>[ad, dropped, nike, nike, air, sesh, white, va...</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.826, 'pos': 0.174, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39417</th>\n",
       "      <td>by any means supreme x nike sb dunk drops toda...</td>\n",
       "      <td>[]</td>\n",
       "      <td>price</td>\n",
       "      <td>adidas</td>\n",
       "      <td>[mean, supreme, x, nike, sb, dunk, drop, today...</td>\n",
       "      <td>{'neg': 0.128, 'neu': 0.305, 'pos': 0.567, 'co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "26586  the nike go flyease surfaces in another multic...   \n",
       "7967             just in nike streetgato white lime glow   \n",
       "1476   for the first time the fruity pebbles lebron f...   \n",
       "4839   nike ikea close russian stores as sanctions tr...   \n",
       "26706  sizes up to one two for the vast grey electric...   \n",
       "47081  so dumb it s ike when people were burning thei...   \n",
       "41527  my number one actually wore em to see batman t...   \n",
       "38498  so good i had to share check out all the items...   \n",
       "38801  ad dropped via nike us nike air sesh white var...   \n",
       "39417  by any means supreme x nike sb dunk drops toda...   \n",
       "\n",
       "                                                hashtags brand_attribute  \\\n",
       "26586                                   ['#sneakersapp']         quality   \n",
       "7967                                                  []         quality   \n",
       "1476                                                  []         quality   \n",
       "4839                                                  []         quality   \n",
       "26706                                                 []           price   \n",
       "47081                                                 []         quality   \n",
       "41527                                                 []         quality   \n",
       "38498  ['#poshmark', '#fashion', '#style', '#shopmycl...         quality   \n",
       "38801                                                 []         quality   \n",
       "39417                                                 []           price   \n",
       "\n",
       "        brand                                              token  \\\n",
       "26586    nike  [nike, flyease, surface, multicolor, sneakersapp]   \n",
       "7967     nike              [nike, streetgato, white, lime, glow]   \n",
       "1476     nike  [time, fruity, pebble, lebron, set, retail, re...   \n",
       "4839     nike  [nike, ikea, close, russian, store, sanction, ...   \n",
       "26706    nike  [size, vast, grey, electric, green, nike, spac...   \n",
       "47081  adidas  [dumb, ike, people, burning, nike, gear, like,...   \n",
       "41527  adidas  [number, actually, wore, em, batman, tuesday, ...   \n",
       "38498  adidas  [good, share, check, item, loving, poshmark, f...   \n",
       "38801  adidas  [ad, dropped, nike, nike, air, sesh, white, va...   \n",
       "39417  adidas  [mean, supreme, x, nike, sb, dunk, drop, today...   \n",
       "\n",
       "                                            vader_scores  \n",
       "26586  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "7967   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "1476   {'neg': 0.0, 'neu': 0.748, 'pos': 0.252, 'comp...  \n",
       "4839   {'neg': 0.208, 'neu': 0.792, 'pos': 0.0, 'comp...  \n",
       "26706  {'neg': 0.0, 'neu': 0.809, 'pos': 0.191, 'comp...  \n",
       "47081  {'neg': 0.525, 'neu': 0.35, 'pos': 0.125, 'com...  \n",
       "41527  {'neg': 0.0, 'neu': 0.645, 'pos': 0.355, 'comp...  \n",
       "38498  {'neg': 0.0, 'neu': 0.383, 'pos': 0.617, 'comp...  \n",
       "38801  {'neg': 0.0, 'neu': 0.826, 'pos': 0.174, 'comp...  \n",
       "39417  {'neg': 0.128, 'neu': 0.305, 'pos': 0.567, 'co...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brands_tweets.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8eb80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915bda9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a64253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e643b46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('are')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f55b9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd642872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Gus\n",
      "is be\n",
      "helping helping\n",
      "organize organize\n",
      "a a\n",
      "developer developer\n"
     ]
    }
   ],
   "source": [
    "sent = 'Gus is helping organize a developer'\n",
    "\n",
    "for token in nlp(sent):\n",
    "    print (token, token.lemma_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
